# ğŸš€ DQN Lunar Lander - Explorando o EspaÃ§o com InteligÃªncia Artificial! ğŸŒ•

Bem-vindo ao **DQN Lunar Lander**, um experimento onde ensinamos um agente a pousar suavemente em um planeta alienÃ­gena! ğŸŒŒğŸ›¸

---

## ğŸ¤– Sobre o Projeto

Neste projeto, utilizamos **Deep Q-Networks (DQN)** para treinar um agente no ambiente *LunarLander-v3*, fornecido pelo Gymnasium. O objetivo? Fazer com que nossa nave pouse de forma segura sem explodir! ğŸ’¥ğŸ”¥

---

## ğŸ“¦ Explicando o Script Linha por Linha

### 1ï¸âƒ£ ImportaÃ§Ã£o das Bibliotecas
```python
import gymnasium as gym
import numpy as np
from stable_baselines3 import DQN
from stable_baselines3.common.evaluation import evaluate_policy
```
O **Gymnasium** fornece o ambiente de simulaÃ§Ã£o, enquanto o **NumPy** Ã© utilizado para operaÃ§Ãµes matemÃ¡ticas e manipulaÃ§Ã£o de arrays. O algoritmo **DQN** aplica aprendizado por reforÃ§o baseado em Q-Learning profundo. Por fim, a funÃ§Ã£o **evaluate_policy** avalia o desempenho do agente apÃ³s o treinamento.

### 2ï¸âƒ£ Criando o Ambiente
```python
env = gym.make("LunarLander-v3", render_mode="rgb_array")
```
O **gym.make()** cria um ambiente de simulaÃ§Ã£o baseado no jogo **Lunar Lander**, permitindo a interaÃ§Ã£o com o ambiente. A opÃ§Ã£o **render_mode="rgb_array"** configura a renderizaÃ§Ã£o para capturar imagens do jogo em formato de array RGB, possibilitando o processamento visual se necessÃ¡rio.

### 3ï¸âƒ£ DefiniÃ§Ã£o do Modelo DQN
```python
model = DQN(
    policy="MlpPolicy",
    env=env,
    verbose=1,
    learning_rate=1e-3,
    buffer_size=100_000,
    learning_starts=10_000,
    batch_size=64,
    gamma=0.99,
    exploration_fraction=0.1,
    exploration_final_eps=0.05,
    train_freq=4,
    target_update_interval=1000,
)
```
A **policy="MlpPolicy"** define uma polÃ­tica de aprendizado baseada em redes neurais MLP (Multilayer Perceptron). O parÃ¢metro **env=env** associa o modelo ao ambiente de simulaÃ§Ã£o. Com **verbose=1**, sÃ£o fornecidas informaÃ§Ãµes sobre o treinamento. A **learning_rate=1e-3** define a taxa de aprendizado, enquanto **buffer_size=100_000** especifica o tamanho do buffer de replay. O aprendizado comeÃ§a apÃ³s **learning_starts=10_000** interaÃ§Ãµes, com **batch_size=64** para o tamanho do lote. O **gamma=0.99** Ã© o fator de desconto para aprendizado futuro, e **exploration_fraction=0.1** regula a exploraÃ§Ã£o. O limite inferior da exploraÃ§Ã£o Ã© dado por **exploration_final_eps=0.05**. A **train_freq=4** indica a frequÃªncia de treinamento, e **target_update_interval=1000** define o intervalo para atualizar a rede-alvo.

### 4ï¸âƒ£ Treinamento do Modelo
```python
print("Iniciando treinamento...")
model.learn(total_timesteps=int(200_000), progress_bar=True)
```
**model.learn()**: Treina o agente por 200.000 timesteps.
**progress_bar=True**: Exibe o progresso do treinamento.

### 5ï¸âƒ£ Salvando o Modelo Treinado
```python
model.save("dqn_lunar")
print("Modelo salvo como dqn_lunar.zip")
```
**model.save("dqn_lunar")**: Salva o modelo treinado em um arquivo ZIP.

### 6ï¸âƒ£ Recarregando o Modelo
```python
del model
model = DQN.load("dqn_lunar", env=env)
print("Modelo carregado!")
```
**del model**: Remove o modelo da memÃ³ria.
**DQN.load("dqn_lunar", env=env)**: Carrega o modelo salvo e associa ao ambiente.

### 7ï¸âƒ£ AvaliaÃ§Ã£o do Agente
```python
mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)
print(f"Recompensa mÃ©dia: {mean_reward:.2f} Â± {std_reward:.2f}")
```
**evaluate_policy()**: Mede o desempenho do agente com 10 episÃ³dios de teste.
 **mean_reward**: MÃ©dia das recompensas obtidas.
**std_reward**: Desvio padrÃ£o das recompensas.

### 8ï¸âƒ£ Executando o Agente Treinado
```python
obs, _ = env.reset()
rewards = []

for i in range(1000):
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, done, _, _ = env.step(action)
    rewards.append(reward)
    env.render()
    
    if done:
        obs, _ = env.reset()
```
**env.reset()**: Reinicia o ambiente.
**model.predict(obs, deterministic=True)**: Escolhe a melhor aÃ§Ã£o baseada na polÃ­tica treinada.
**env.step(action)**: Aplica a aÃ§Ã£o ao ambiente.
**env.render()**: Renderiza a simulaÃ§Ã£o.
**if done:** Reinicia o ambiente ao final do episÃ³dio.

### 9ï¸âƒ£ Fechando o Ambiente
```python
env.close()
```
- **env.close()**: Fecha a simulaÃ§Ã£o e libera os recursos.

---

## ğŸš€ Como Rodar o Projeto?

### 1ï¸âƒ£ Instale as dependÃªncias
```bash
pip install gymnasium numpy stable-baselines3
```

### 2ï¸âƒ£ Execute o script
```bash
python dqn_lunar.py
```

### 3ï¸âƒ£ Assista ao pouso (ou explosÃ£o)!
Se tudo der certo, seu agente pousarÃ¡ suavemente. Se nÃ£o... bem, foguetes sÃ£o difÃ­ceis de pilotar! ğŸ˜†

---

## ğŸ† ConclusÃ£o
Este README explica cada linha do script e como o Python interpreta os comandos. Agora, vocÃª pode entender e modificar o cÃ³digo para melhor desempenho ou criar novas versÃµes! ğŸš€ğŸ˜ƒ

Divirta-se e bons experimentos! ğŸ§ª

