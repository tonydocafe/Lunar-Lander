# üöÄ DQN Lunar Lander - Explorando o Espa√ßo com Intelig√™ncia Artificial! üåï

Bem-vindo ao **DQN Lunar Lander**, um experimento onde ensinamos um agente a pousar suavemente em um planeta alien√≠gena! üååüõ∏

---

## ü§ñ Sobre o Projeto

Neste projeto, utilizamos **Deep Q-Networks (DQN)** para treinar um agente no ambiente *LunarLander-v3*, fornecido pelo Gymnasium. O objetivo? Fazer com que nossa nave pouse de forma segura sem explodir! üí•üî•

---

## üì¶ Explicando o Script Linha por Linha

### 1Ô∏è‚É£ Importa√ß√£o das Bibliotecas
```python
import gymnasium as gym
import numpy as np
from stable_baselines3 import DQN
from stable_baselines3.common.evaluation import evaluate_policy
```
O **Gymnasium** fornece o ambiente de simula√ß√£o, enquanto o **NumPy** √© utilizado para opera√ß√µes matem√°ticas e manipula√ß√£o de arrays. O algoritmo **DQN** aplica aprendizado por refor√ßo baseado em Q-Learning profundo. Por fim, a fun√ß√£o **evaluate_policy** avalia o desempenho do agente ap√≥s o treinamento.

### 2Ô∏è‚É£ Criando o Ambiente
```python
env = gym.make("LunarLander-v3", render_mode="rgb_array")
```
O **gym.make()** cria um ambiente de simula√ß√£o baseado no jogo **Lunar Lander**, permitindo a intera√ß√£o com o ambiente. A op√ß√£o **render_mode="rgb_array"** configura a renderiza√ß√£o para capturar imagens do jogo em formato de array RGB, possibilitando o processamento visual se necess√°rio.

### 3Ô∏è‚É£ Defini√ß√£o do Modelo DQN
```python
model = DQN(
    policy="MlpPolicy",
    env=env,
    verbose=1,
    learning_rate=1e-3,
    buffer_size=100_000,
    learning_starts=10_000,
    batch_size=64,
    gamma=0.99,
    exploration_fraction=0.1,
    exploration_final_eps=0.05,
    train_freq=4,
    target_update_interval=1000,
)
```
A **policy="MlpPolicy"** define uma pol√≠tica de aprendizado baseada em redes neurais MLP (Multilayer Perceptron). O par√¢metro **env=env** associa o modelo ao ambiente de simula√ß√£o. Com **verbose=1**, s√£o fornecidas informa√ß√µes sobre o treinamento. A **learning_rate=1e-3** define a taxa de aprendizado, enquanto **buffer_size=100_000** especifica o tamanho do buffer de replay. O aprendizado come√ßa ap√≥s **learning_starts=10_000** intera√ß√µes, com **batch_size=64** para o tamanho do lote. O **gamma=0.99** √© o fator de desconto para aprendizado futuro, e **exploration_fraction=0.1** regula a explora√ß√£o. O limite inferior da explora√ß√£o √© dado por **exploration_final_eps=0.05**. A **train_freq=4** indica a frequ√™ncia de treinamento, e **target_update_interval=1000** define o intervalo para atualizar a rede-alvo.

### 4Ô∏è‚É£ Treinamento do Modelo
```python
print("Iniciando treinamento...")
model.learn(total_timesteps=int(200_000), progress_bar=True)
```
**model.learn()**: Treina o agente por 200.000 timesteps.
**progress_bar=True**: Exibe o progresso do treinamento.

### 5Ô∏è‚É£ Salvando o Modelo Treinado
```python
model.save("dqn_lunar")
print("Modelo salvo como dqn_lunar.zip")
```
**model.save("dqn_lunar")**: Salva o modelo treinado em um arquivo ZIP.

### 6Ô∏è‚É£ Recarregando o Modelo
```python
del model
model = DQN.load("dqn_lunar", env=env)
print("Modelo carregado!")
```
**del model**: Remove o modelo da mem√≥ria.
**DQN.load("dqn_lunar", env=env)**: Carrega o modelo salvo e associa ao ambiente.

### 7Ô∏è‚É£ Avalia√ß√£o do Agente
```python
mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)
print(f"Recompensa m√©dia: {mean_reward:.2f} ¬± {std_reward:.2f}")
```
**evaluate_policy()**: Mede o desempenho do agente com 10 epis√≥dios de teste.
 **mean_reward**: M√©dia das recompensas obtidas.
**std_reward**: Desvio padr√£o das recompensas.

### 8Ô∏è‚É£ Executando o Agente Treinado
```python
obs, _ = env.reset()
rewards = []

for i in range(1000):
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, done, _, _ = env.step(action)
    rewards.append(reward)
    env.render()
    
    if done:
        obs, _ = env.reset()
```
**env.reset()**: Reinicia o ambiente.
**model.predict(obs, deterministic=True)**: Escolhe a melhor a√ß√£o baseada na pol√≠tica treinada.
**env.step(action)**: Aplica a a√ß√£o ao ambiente.
**env.render()**: Renderiza a simula√ß√£o.
**if done:** Reinicia o ambiente ao final do epis√≥dio.

### 9Ô∏è‚É£ Fechando o Ambiente
```python
env.close()
```
- **env.close()**: Fecha a simula√ß√£o e libera os recursos.

---

## üöÄ Como Rodar o Projeto?

### 1Ô∏è‚É£ Instale as depend√™ncias
```bash
pip install gymnasium numpy stable-baselines3
```

### 2Ô∏è‚É£ Execute o script
```bash
python3 aprendizado.py
```

### 3Ô∏è‚É£ Assista ao pouso (ou explos√£o)!
Se tudo der certo, seu agente pousar√° suavemente. Se n√£o... bem, foguetes s√£o dif√≠ceis de pilotar! üòÜ

```bash
python3 visualiza√ß√£o.py
```

---
                                      

![Imagem Exemplo](etc/dqn_1.png)
-
![Imagem Exemplo](etc/dqn_2.png)
-
![Imagem Exemplo](etc/dqn_1.png)
-
