1-Environment:

* I used a python environment where I installed all dependencies

python3 -m venv environment
source environment/bin/activate

2-Parameters:

- Changes for results analysis -

* number of timesteps

{ This parameter defines how many interactions the AI ​​will have with the environment during training. It learns from interactions with the environment, adjusting its policy based on the rewards received.}

model.learn(total_timesteps=int(200_000), progress_bar=True)

* learning rate (learning_rate)

{ Defines the size of the adjustment in the neural network weights in each update. It adapts faster with high values, but it can fluctuate; low values ​​make learning more stable, but slower.}

model = DQN(...learning_rate=1e-3,...)

* discount factor (gamma)

{Determines the weight of future rewards in the agent's decision. High values ​​make the agent plan in the long term; low values ​​make it act more immediately.}

model = DQN(...gamma=0.99,...)

* episodes in the evaluation

{Number of episodes executed to evaluate the agent's performance after training. It does not learn during the evaluation, it only executes actions with the trained policy.}

mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)

This ensures that the evaluation does not depend on a single episode and has a more stable metric.

3-Outputs

- Trained model

* dqn_lunar_0 (first test - default)

----------------------------------
| rollout/ | |
| ep_len_mean | 829 |
| ep_rew_mean | 4.84 |
| exploration_rate | 0.05 |
| time/ | | | episodes | 328 |
| fps | 205 |
| time_elapsed | 970 |
| total_timesteps | 199128 |
| train/ | |
| learning_rate | 0.001 |
| loss | 0.474 |
| n_updates | 47281 | ----------------------------------

Average reward: -128.26 ± 73.87

timestep - 200_000
learning_rate - 1e-3(1 * 10^(-3) = 0.001)
gamma - 0.99
n_eval_episodes - 10



* dqn_lunar_1 (second test - wide)

----------------------------------
| rollout/ | |
| ep_len_mean | 238 |
| ep_rew_mean | -381 |
| exploration_rate | 0.05 |
| time/ | |
| episodes | 1800 |
| fps | 323 |
| time_elapsed | 1540 |
| total_timesteps | 498805 |
| train/ | |
| learning_rate | 0.005 |
| loss | 3.26 |
| n_updates | 122201 |
----------------------------------

Average reward: -364.51 ± 50.96


timestep - 500_000
learning_rate - 5e-3 (5 * 10^(-3) = 0.005)
gamma - 0.999
n_eval_episodes - 50

* dqn_lunar_2 (third test - defined)

----------------------------------
| rollout/ | |
| ep_len_mean | 390 |
| ep_rew_mean | -246 |
| exploration_rate | 0.05 |
| time/ | |
| episodes | 200 |
| fps | 323 |
| time_elapsed | 150 |
| total_timesteps | 48687 |
| train/ | |
| learning_rate | 0.0001 |
| loss | 0.914 |
| n_updates | 9671 |
----------------------------------

Average reward: -91.90 ± 29.75

timestep - 50_000
learning_rate - 1e-4 (1 * 10^(-4) = 0.0001)
gamma - 0.85

n_eval_episodes - 5

file changes were made in :

model.save("...")
print("...")
...
model = DQN.load("...",env=env)
